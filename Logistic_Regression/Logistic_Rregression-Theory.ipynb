{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Questions - **Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. What is Logistic Regression, and how does it differ from Linear Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Logistic Regression** is a statistical techniques used for classification dataset(primarily used for *binary*) prediction, it predicts the probability that an observation belongs to a particular class (e.g., 0 or 1). *Unlike Linear Regression, which predicts continuous outcomes* (e.g., house prices), Logistic Regression outputs probabilities bounded between 0 and 1, making it suitable for categorical outcomes.\n",
    "\n",
    "**Key Differences:**\n",
    "- **Type of Output:** Linear Regression predicts a *continuous value* (e.g., ( y = -2.5) or ( 100 )), while Logistic Regression predicts a probability (e.g., $( P(y=1) = 0.75 )$), using the *sigmoid function*, and decides the class according to the probability.\n",
    "- **Model Equation:** Linear Regression uses: $( y = \\theta_0 + \\theta_1x )$, while Logistic Regression models the log-odds: \n",
    "  > $ \\log\\left(\\frac{p}{1-p}\\right) = \\theta_0 + \\theta_1x $ <br> and ultimately the equation gets down to, <br> $y = \\frac{1}{1 + {e^{-(\\theta_0 + \\theta_1 x)}}}$\n",
    "- **Purpose:** Linear Regression is for regression tasks; Logistic Regression is for classification.\n",
    "- **Example:** Predicting temperature, prices (Linear) vs. predicting if it will rain (yes/no), stock prices will go up or down. (Logistic).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. What is the mathematical equation of Logistic Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression models the probability of a binary outcome using the logistic (sigmoid) function. The equation relates the linear combination of features to a probability:\n",
    "\n",
    "> <br> $P(y=1|x) = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_ nx_n)}} = y$ <br> .\n",
    "Where:\n",
    "- $P(y=1|x)$: Probability of the positive class given features \\( x \\).\n",
    "- $(\\theta_0)$: Intercept.\n",
    "- $( \\theta_1, \\theta_2, \\dots, \\theta_n)$: Coefficients for features $( x_1, x_2, \\dots, x_n).$\n",
    "- $(e)$: Base of the natural logarithm.\n",
    "\n",
    "\n",
    "**Intuition:** The linear part $( \\beta_0 + \\beta_1x )$ ['which is actually the simple linear regression] can range from $(-\\infty, infty)$, and the sigmoid squashes it into the range [0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Why do we use the Sigmoid function in Logistic Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sigmoid function, $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, is used because it transforms a linear combination of inputs (continuous) into a probability between 0 and 1. This matches the goal of binary classification: estimating $P(y=1)$. Which is the sigmoid function is the main differnce which squashes the continuous curve into 0,1.\n",
    " where z is the SLR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. What is the cost function of Logistic Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function for Logistic Regression is the **Log Loss** also called **Cross-Entropy Loss**, which measures the difference between predicted probabilities and actual labels. \n",
    "\n",
    "> For a binary classification:\n",
    "$J(\\theta) = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$\n",
    "\n",
    "Where:\n",
    "- $(n)$: Number of samples.\n",
    "- $(y_i)$: True label (0 or 1).\n",
    "- $\\hat{y}_i$ = $\\sigma(\\theta_0 + \\theta_1 x_1)$: Predicted probability. And $\\sigma$ is the sigmoid function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. What is Regularization in Logistic Regression? Why is it needed?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization adds a 'penalty' term to the cost function to reduce overfitting (Using Ridge or L2) or for feature selection (we use Lasso or L1) by discouraging overly complex models (large coefficients).\n",
    "\n",
    "**Modified Cost Function:**\n",
    "$J(\\theta) = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] + \\lambda_1[\\sum_{i}\\theta_{i}^{2}] + \\lambda{2}[\\sum_{i}{|\\theta_i|}]$\n",
    "\n",
    "$\\lambda$'s are the controls its strength. This is the regularization for elastic net where first part is reasonable for the ridge and absolute part is for the lasso.\n",
    "\n",
    "**Why Needed:**\n",
    "- **Overfitting:** L2 regularization, helps to reduce the overfitting in the model.\n",
    "- **Feature Selection:** L1 (Lasso) shrinks some coefficients to zero, which might be insignificant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Explain the difference between Lasso, Ridge, and Elastic Net regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Lasso (L1):**\n",
    "  - Penalty: $\\sum |\\theta_j|$\n",
    "  - Shrinks some coefficients to exactly 0 which are insignificant to the target, which helps in feature selection.\n",
    "\n",
    "\n",
    "- **Ridge (L2):**\n",
    "  - Penalty: $\\sum \\theta_j^2$\n",
    "  - Shrinks coefficients toward 0 but not exactly 0, helps in reducing overfitting.\n",
    "\n",
    "- **Elastic Net:**\n",
    "  - Penalty: $\\alpha \\sum |\\beta_j| + (1 - \\alpha) \\sum \\beta_j^2$ (It's the mix of L1 and L2).\n",
    "  - Combines feature selection (L1) and coefficient shrinkage (L2).\n",
    "  - It can be used when features are correlated and we are not sure about which regularization to use. It basically balances both the L1 and L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. When should we use Elastic Net instead of Lasso or Ridge?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use **Elastic Net** when:\n",
    "- **Correlated Features:** Lasso might arbitrarily select one feature from a correlated group, while Elastic Net handles this better by combining L1 and L2. Elastic net handles multi-collinearity better.\n",
    "- Elastic can be useful when we have large number of features so that by regularization we can do both shrink some coefficients and making some of the coefficients to exactly 0.\n",
    "\n",
    "- And In general elastic net might not be much useful for small datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8. What is the impact of the regularization parameter (λ) in Logistic Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularization parameter $(\\lambda)$ (or, $C = 1/\\lambda$) controls the strength of the penalty:\n",
    "\n",
    "- **Large $\\lambda $ Small C** indicates a Strong regularization.\n",
    "- Coefficients shrink more (many become 0 with L1).\n",
    "- Simpler model, less overfitting, but risks of underfitting.\n",
    "\n",
    "- usually we use $\\lambda = 1$ or we might use hypertuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **9. What are the key assumptions of Logistic Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Binary or categorical Outcome:** While logistic regression is primarily used for binary classification so the target variable should be binary (e.g., 0 or 1). For example predicting pass/fail or yes/no.\n",
    "2. **Independence:** Each observation should be independent of each other, and not influenced by others. For example, one student’s test score shouldn’t affect another’s.\n",
    "3. **Linearity in Log-Odds:** The relationship between features and the log-odds (also known as logit) $\\log\\left(\\frac{p}{1-p}\\right)$ should be linear. This doesn’t mean the probability itself is linear, but the transformed odds are.\n",
    "4. **No Multicollinearity:** Features shouldn’t be highly correlated among themselves. If study hours and prep time are nearly identical, it confuses the model about which matters more. (that's where regularization comes in)\n",
    "5. **Large Sample Size:** We need decent amount of data, especially if one class is rare, that is for imbalanced data and not fixed with proper EDA might lead to misleading inferences.\n",
    "\n",
    "- Unlike Linear Regression, we don’t need the normality assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **10. What are some alternatives to Logistic Regression for classification tasks?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The most closest alternative to logistic regression can be **Probit regression**, which basically uses the standard normal distribution instead of the bernoulli as a link function.\n",
    "Some other classification algorithms like:\n",
    "\n",
    "- **Decision Trees:** Here we split data into branches based on feature thresholds. It can be easy to visualize.\n",
    "- **Random Forests:** A team of decision trees voting together. They’re robust and handle messy data well, though less interpretable.\n",
    "- **Support Vector Machines (SVM):** We find the best boundary (hyperplane) to separate classes. With kernels, which might be good to tackle non-linear problems.\n",
    "- **K-Nearest Neighbors (KNN):** This looks at the closest k data points to decide the target category.\n",
    "- **Naive Bayes:** This use probability and assumes features are independent. It’s fast for text tasks like spam filtering.\n",
    "- **Neural Networks:** This mighht be the most advanced algorithm among these and can captures complex patterns (e.g., image recognition), but they need lots of data and tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **11. What are Classification Evaluation Metrics?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Confusion Matrix:** A table showing the counts for True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
    "- **Accuracy:** The percentage of correct predictions from all data points. $\\frac{TP + TN}{TP + TN + FP + FN}$. A good metric to interpret but we don't solely rely on accuracy only. We need to verify the other metrics too.\n",
    "- **Precision:** How many predicted positives were actually positive $\\frac{TP}{TP + FP}$. It is a key metric when a large FP can be critical in real life scenarios. (e.g., spam filterations).\n",
    "- **Recall (Sensitivity):** How many actual positives are there from all the predicted. $\\frac{TP}{TP + FN}$. It is Critical when False Negatives should be avoided. (e.g., disease detection).\n",
    "- **F-beta-Score:** Balances precision and recall $(1 + \\beta^2) \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$. Perfect for imbalanced data. Where, $\\beta = 1 or 0.5 or 2$ when FP and FN both are critical, or FP is more critical or FN is more critical respectively.\n",
    "- **ROC-AUC:** Plots true vs. false positive rates (TPR V. FPR). A higher area under the curve indicates a better categorization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **12. How does class imbalance affect Logistic Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class imbalance, meanse when one class is dominating the complete dataset (e.g., 90% no , 10% yes) this can be tricky and bias driven for our Logistic Regression model. Intuitively if we think, even if the model gives all of the output as 'no' the model accuracy will still show approximately 90% which is misleading. So the affects are-\n",
    "\n",
    "- **Bias Toward Majority:** The model might just predict the majority class (e.g., \"no\") to look good on accuracy, ignoring the minority.\n",
    "- **Skewed Probabilities:** Coefficients get tuned to the bigger class, making it less sensitive to the rare one.\n",
    "- **Cost Function Issue:** Our standard cost fn, <center> <br> $J(\\theta) = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$ </center>, treats all errors equally, so it’s happy optimizing for the majority.\n",
    "\n",
    "**Fixes:**\n",
    "- We can add class weights to penalize minority errors more.\n",
    "- Resampling techniques like Oversample the minority or undersample the majority in EDA.\n",
    "- we can change the decision threshold (e.g., from 0.5 to 0.3).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **13. What is Hyperparameter Tuning in Logistic Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning is the technique to find out the best combination of parameters to use to get the best response from our model. it's like tweaking the settings of our machine to get the best performance. In Logistic Regression, we adjust values set before training, we can use various parameters to tune:\n",
    "\n",
    "- **C = (Inverse of Regularization Strength):** Controls how much we penalize big coefficients (small C = strong penalty).\n",
    "- **Penalty:** To find out which regularization working best for our dataset. L1 (Lasso), L2 (Ridge), or Elastic Net.\n",
    "- **Solver:** The algorithm solving the optimization (e.g., liblinear, saga, lbfgs(default)).\n",
    "\n",
    "**Methods of Tuning:** Here cross validations are also gets applied, the train data splits further into training data and validation data.\n",
    "- **Grid Search CV:** Tries every combinations given as parameters in a range (e.g., C = [0.1, 1, 10]).\n",
    "- **Random Search CV:** This picks random combinations and it works faster for big ranges.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **14. What are different solvers in Logistic Regression? Which one should be used?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solvers are the mechanisms that minimize our cost function. Here’s the rundown:\n",
    "\n",
    "- **liblinear:** Uses coordinate descent. Good for small datasets and supports L1 or L2 penalties. Doesn't work for multiclass\n",
    "- **lbfgs:** Default solver in scikit learn. An approximation of Newton’s method. Efficient for medium datasets with L2.\n",
    "- **newton-cg:** Full Newton’s method with conjugate gradient. Accurate but heavy for L2.\n",
    "- **sag:** Stochastic Average Gradient. Fast for big datasets with L2.\n",
    "- **saga:** Like SAG but supports L1, L2, and Elastic Net. Super versatile.\n",
    "\n",
    "\n",
    "- Logistic Regression Solvers Comparison\n",
    "\n",
    "| Solver      | Type of Optimization  | Supports L1 | Supports L2 | Multiclass Support | Best Use Case |\n",
    "|------------|----------------------|-------------|-------------|------------------|---------------|\n",
    "| **lbfgs**  | Quasi-Newton (2nd order) | ❌ No | ✅ Yes | ✅ Yes (Softmax) | Best for **multiclass classification** |\n",
    "| **liblinear** | Coordinate Descent (1st order) | ✅ Yes | ✅ Yes | ❌ No (OvR only) | Best for **small datasets, L1 regularization** |\n",
    "| **saga**   | Stochastic Average Gradient (1st order) | ✅ Yes | ✅ Yes | ✅ Yes (Softmax) | Best for **large datasets & L1 regularization** |\n",
    "| **sag**    | Stochastic Average Gradient (1st order) | ❌ No | ✅ Yes | ✅ Yes (Softmax) | Best for **large datasets & fast convergence** |\n",
    "| **newton-cg** | Newton’s Method (2nd order) | ❌ No | ✅ Yes | ✅ Yes (Softmax) | Best for **L2-regularized problems** |\n",
    "\n",
    "**Which one to use**\n",
    "- **`lbfgs` (also the default) for most problems (multiclass & L2 regularization).**\n",
    "- **`saga` for L1 or Elastic Net regularization.**\n",
    "- **`liblinear` for small datasets with L1 regularization.**\n",
    "- **`sag` for massive datasets with L2.**\n",
    "- **`newton-cg` when needed second-order optimization for L2 regularization.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **15. How is Logistic Regression extended for multiclass classification?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Logistic Regression is primarily for **binary classification**, but it can be extended to handle **multiclass classification** using two main methods:\n",
    "\n",
    "**1️ One-vs-Rest (OvR)**\n",
    "- Trains **K binary classifiers** (one per class).\n",
    "- For each classifier, one class is **positive (1)**, and all others are **negative (0)**.\n",
    "- And The class with the **highest probability** is chosen.\n",
    "- Works with **all solvers** (e.g., liblinear, lbfgs, saga).\n",
    "- It's simple but might not be efficient for large datasets since it classifies 1 data point at an iteration.\n",
    "\n",
    "**2 Softmax Regression (Multinomial Logistic Regression)**\n",
    "- It uses the concept of Multinomial distribution which is also an extension of the Binomial distribution.\n",
    "- Directly models **all K classes** using the **Softmax function**.\n",
    "- Uses a **single model** instead of K binary classifiers.\n",
    "- This multi_class technique only works with some **solvers supporting multinomial logistic regression** (e.g., lbfgs, saga, newton-cg).\n",
    "\n",
    "**Mathematical Representation:**\n",
    "For class \\( k \\), the probability is:\n",
    "\n",
    "\n",
    "\n",
    "**Key difference**\n",
    "| Scenario | Method to use |\n",
    "|----------|------------------|\n",
    "| Small datasets or `liblinear` solver | **One-vs-Rest (OvR)** |\n",
    "| Large datasets, Softmax required | **Multinomial (Softmax Regression)** |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **16. What are the advantages and disadvantages of Logistic Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Logistic Regression is a widely used and one of the most popular classification algorithm because it is simple yet effective for many real-world problems. However, there are some limitations as well.\n",
    "\n",
    "\n",
    "**Advantages of Logistic Regression**\n",
    "1. **Simple and Interpretable**  \n",
    "   - Easy to implement and interpret compared to other complex models.\n",
    "   - The coefficients indicate feature importance.\n",
    "\n",
    "2. **Probabilistic Output**  \n",
    "   - Outputs class probabilities instead of just predictions. (We can see the probabilites in sckit learn as well using the model.proba method)\n",
    "   - It implements the threshold-based decision-making.\n",
    "\n",
    "3. **Efficient and Fast**  \n",
    "   - Works well on small to medium-sized datasets.\n",
    "   - Training and inference are computationally efficient.\n",
    "\n",
    "4. **Works Well with Linearly Separable Data**  \n",
    "   - If data is linearly separable, Logistic Regression performs well.\n",
    "\n",
    "5. **Handles Multiclass Classification**  \n",
    "   - Supports **One-vs-Rest (OvR)** and **Softmax Regression (multinomial)** for multiclass problems.\n",
    "\n",
    "6. **Regularization Support (L1 & L2)**  \n",
    "   - L1 (Lasso) for feature selection.\n",
    "   - L2 (Ridge) for reducing overfitting.\n",
    "\n",
    "\n",
    "**Disadvantages of Logistic Regression**\n",
    "1. **Assumes Linear Decision Boundary**  \n",
    "   - Cannot model complex relationships in non-linear data.\n",
    "   - Needs transformation (polynomial features) or kernel tricks.\n",
    "\n",
    "2. **Sensitive to Outliers**  \n",
    "   - Outliers can heavily affect parameter estimation.\n",
    "\n",
    "3. **Requires Feature Engineering**  \n",
    "   - Needs careful feature selection and scaling for optimal performance.\n",
    "   - Performance depends on choosing the right independent variables.\n",
    "\n",
    "4. **Not Suitable for Large Feature Spaces**  \n",
    "   - Computationally inefficient for high-dimensional data.\n",
    "\n",
    "5. **Cannot Handle Highly Correlated Features (Multicollinearity)**  \n",
    "   - Multicollinearity can distort coefficient estimates.\n",
    "   - We use **feature selection or PCA** to address the issue.\n",
    "\n",
    "\n",
    "**When to Use Logistic Regression?**\n",
    "| Scenario | Logistic Regression |\n",
    "|----------|----------------------|\n",
    "| Binary Classification (0/1) | Yes |\n",
    "| Multiclass Classification (OvR/Softmax) | Yes |\n",
    "| Large, high-dimensional data | No (Consider SVM, Neural Networks) |\n",
    "| Linearly Separable Data | Yes |\n",
    "| Complex Non-Linear Relationships | No (Consider Decision Trees, Neural Networks) |\n",
    "| Need Probabilistic Outputs | Yes |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## visualization and example for linearly separable data\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.datasets import make_classification, make_moons\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# # Generate linearly separable data\n",
    "# X_linear, y_linear = make_classification(n_samples=100, n_features=4, n_clusters_per_class=2, n_classes=2, random_state=42)\n",
    "\n",
    "# # Generate non-linearly separable data\n",
    "# X_nonlinear, y_nonlinear = make_moons(n_samples=100, noise=0.2, random_state=42)\n",
    "\n",
    "# # Plot both datasets\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# # Linearly Separable\n",
    "# ax[0].scatter(X_linear[:, 0], X_linear[:, 1], c=y_linear, cmap='bwr', edgecolors='k')\n",
    "# ax[0].set_title(\"Linearly Separable Data\")\n",
    "\n",
    "# # Not Linearly Separable\n",
    "# ax[1].scatter(X_nonlinear[:, 0], X_nonlinear[:, 1], c=y_nonlinear, cmap='bwr', edgecolors='k')\n",
    "# ax[1].set_title(\"Non-Linearly Separable Data\")\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **17. What are some use cases of Logistic Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Logistic Regression is widely used for **classification tasks** especially **binary** where the goal is to predict discrete categorical outcomes. Some common applications include:\n",
    "\n",
    "**1️ Medical Diagnosis**\n",
    "- Predicting diseases (Yes/No. Diabetic/Non-diabetic).\n",
    "- Classifying patients as **high-risk or low-risk**.\n",
    "\n",
    "**2️ Fraud Detection or Spam filter**\n",
    "- Identifying **fraudulent transactions** in banking, (or True caller showing fraud alert)\n",
    "- Detecting **spam emails** in email filtering.\n",
    "\n",
    "**3️ Customer Churn Prediction**\n",
    "- Predicting whether a **customer will leave a service** (e.g., telecom, SaaS platforms).\n",
    "\n",
    "**4️ Credit Scoring & Loan Approval**\n",
    "- Banks use it to determine **loan approval** based on customer history.\n",
    "\n",
    "**5️ Marketing & Ad Click Prediction**\n",
    "- Predicting whether a user will **click on an ad** (CTR prediction) (or, will the customer buy the product).\n",
    "- Classifying customers for **targeted marketing campaigns**.\n",
    "\n",
    "**6️ Sentiment Analysis**\n",
    "- Classifying text as **positive, neutral, or negative**.\n",
    "- Common in **social media analysis** and **customer reviews**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **18. What is the difference between Softmax Regression and Logistic Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression and Softmax Regression are both classification algorithms, but they differ in how they handle the **number of classes** as Softmax uses Multinomial which is for Multiclass problems. This is the key difference.\n",
    "\n",
    "| **Feature**        | **Logistic Regression** | **Softmax Regression** |\n",
    "|--------------------|------------------------|------------------------|\n",
    "| **Used For**       | Binary Classification (0 or 1) | Multiclass Classification (3+ classes) |\n",
    "| **Decision Boundary** | Single threshold at **0.5** | Selects the class with the highest probability |\n",
    "| **Model Type**     | One decision function for 2 classes | A separate function for each class |\n",
    "| **Solver Support** | Works with **all solvers** | Needs `lbfgs`, `saga`, or `newton-cg` |\n",
    "\n",
    "- **we use Logistic Regression** when there are **only two classes (binary classification)**.\n",
    "- **we use Softmax Regression** when there are **multiple classes (multiclass classification)**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Both **One-vs-Rest (OvR)** and **Softmax (Multinomial Logistic Regression)** are used for **multiclass classification**, but their choice depends on dataset size, solver compatibility, and efficiency.\n",
    "\n",
    "**One-vs-Rest (OvR)**\n",
    "- Trains **K binary classifiers** (one for each *class vs. the rest*).\n",
    "- The class with the **highest probability** wins.\n",
    "> **Advantages:**  \n",
    "- Works with **all solvers** (including `liblinear`).\n",
    "- Faster for **very large datasets** with **many classes**.\n",
    "- Easier to interpret.\n",
    "> **Disadvantages:**  \n",
    "- Requires training **K models**, increases time complexity.\n",
    "- May have **inconsistent probability estimates**.\n",
    "\n",
    "**Softmax (Multinomial Logistic Regression)**\n",
    "- Uses a **single model** to compute probabilities for all classes.\n",
    "- Relies on the **Softmax function** to assign probabilities.\n",
    "\n",
    "**Advantages:**  \n",
    "- **More efficient** than OvR for smaller datasets.\n",
    "- Provides **better probability calibration**.\n",
    "\n",
    "**Disadvantages:**  \n",
    "- Only works with specific solvers (`lbfgs`, `saga`, `newton-cg`).\n",
    "- May not scale well for datasets with **many classes (K >> 10)**.\n",
    "\n",
    "**Summary?**\n",
    "| Scenario | Recommended Approach |\n",
    "|----------|----------------------|\n",
    "| Small dataset | **Softmax (Multinomial)** |\n",
    "| Large dataset, many classes (K > 10) | **OvR** |\n",
    "| Need probability calibration | **Softmax (Multinomial)** |\n",
    "| Using `liblinear` solver | **OvR** |\n",
    "| Faster training for high-dimensional data | **OvR** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **20. How do we interpret coefficients in Logistic Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In Logistic Regression, the coefficients $\\theta$ represent the **log-odds change** in the dependent variable(y) for a **one-unit increase** in the predictor variable(X).\n",
    "\n",
    "\n",
    "**Understanding the Coefficients**\n",
    "For a given feature \\( x_i \\), the probability of the positive class is:\n",
    "\n",
    "$P(y=1|x) = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 x_1 + \\dots + \\theta_n x_n)}}$\n",
    "\n",
    "\n",
    "- $\\theta_j \\text{the coefficient for feature}, x_j$  tells us how much the **log-odds** of the positive class changes when $x_j$ increases by 1 unit.\n",
    "\n",
    "- The log-odds (logit) transformation:\n",
    "\n",
    "\n",
    "$\\log\\left(\\frac{P(y=1)}{P(y=0)}\\right) = \\theta_0 + \\theta_1 x_1 + \\dots + \\theta_n x_n$\n",
    "\n",
    "**Interpreting Coefficients in Terms of Odds Ratio**\n",
    "- The exponentiated coefficient \\( e^{\\theta_j} \\) gives the **odds ratio**.\n",
    "- If \\( e^{\\theta_j} > 1 \\), the feature **increases** the likelihood of the positive class.\n",
    "- If \\( e^{\\theta_j} < 1 \\), the feature **decreases** the likelihood of the positive class.\n",
    "- If \\( \\theta_j = 0 \\), the feature has **no effect**.\n",
    "\n",
    "**Example:**  \n",
    "If $\\theta_1 = 0.7$, then $e^{0.7} \\approx 2.01$, meaning a **one-unit increase in  $x_1$ doubles the odds** of $ y=1$.\n",
    "\n",
    "\n",
    "\n",
    "**Multiclass Logistic Regression**\n",
    "- For **multiclass classification (Softmax)**, each class has a separate set of coefficients.\n",
    "- Interpretation is done relative to a **reference class**.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
