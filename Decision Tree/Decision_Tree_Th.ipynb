{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Decision Tree - Theoretical Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **1. What is a Decision Tree, and how does it work?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A **Decision Tree** is a supervised machine learning algorithm which can be used for both classification and regression tasks.\n",
        "It works by recursively splitting the dataset into branches based on feature values, forming a tree-like structure, which is a type of Greedy Algorithm as it keep on splitting the branches until the leaf node becomes completely pure (Homogeneous).\n",
        "\n",
        "Each internal node represents a decision on a feature, each branch represents the outcome of a decision, and each leaf represents a class label or prediction result.\n",
        "\n",
        "The goal is to create a model that predicts the target variable by learning simple decision rules inferred from data features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **2. What are impurity measures in Decision Trees?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Impurity measures** quantify the level of disorder or impurity in a dataset. These helps to determine the quality of a split in the dataset.\n",
        "\n",
        "Common impurity measures include:\n",
        "- **Gini Impurity**\n",
        "- **Entropy (Information Gain)**\n",
        "\n",
        "Lower impurity implies a better split that leads to more homogeneous subgroups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **3. What is the mathematical formula for Gini Impurity?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Gini Impurity is calculated using the formula:\n",
        "\n",
        "$$\t\\text{Gini} = 1 - \\sum_{i=1}^{n} p_i^2$$\n",
        "\n",
        "Where:\n",
        "- $n$ = number of classes\n",
        "- $p_i$ = proportion of class $i$ elements in the node\n",
        "\n",
        "For binary classification, this becomes:\n",
        "$$\t\\text{Gini} = 2p(1-p)$$ \n",
        "where $p$ is the probability of one class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **4. What is the mathematical formula for Entropy?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The formula for **Entropy** is:\n",
        "\n",
        "$$\t\\text{H}(S) = - \\sum_{i=1}^{n} p_i \\cdot log_2(p_i)$$\n",
        "\n",
        "Where:\n",
        "- $H$ = Represents Entropy\n",
        "- $n$ = number of classes\n",
        "- $p_i$ = proportion of class $i$ in set $S$\n",
        "\n",
        "Entropy reaches its maximum when the classes are perfectly balanced, and is zero when all data belongs to a single class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **5. What is Information Gain, and how is it used in Decision Trees?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Information Gain (IG)** measures the reduction in entropy or impurity after a dataset is split on a feature, and it also help us to decide which feature to choose for the root node to split further.\n",
        "\n",
        "It is calculated as:\n",
        "$$IG(S, A) = H(S) - \\sum_{v \\in \\text{ val(A)}} \\frac{|S_v|}{|S|} \\cdot H(S_v)$$\n",
        "\n",
        "Where $S$ is the dataset, $A$ is the feature, and $S_v$ is the subset of $S$ for which feature $A$ has value $v$.\n",
        "\n",
        "The feature with the highest IG is chosen for splitting at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **6. What is the difference between Gini Impurity and Entropy?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Criterion        | Gini Impurity                     | Entropy                             |\n",
        "|------------------|----------------------------------|--------------------------------------|\n",
        "| Formula          | $1 - \\sum p_i^2$                 | $- \\sum p_i \\cdot log_2 p_i$               |\n",
        "| Interpretation   | Probability of misclassification| Amount of information (uncertainty) |\n",
        "| Computationally  | Faster                          | Slower due to logarithms             |\n",
        "| Usage            | CART algorithm                   | ID3, C4.5 algorithms                  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **7. What is the mathematical explanation behind Decision Trees?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Decision Trees aim to partition the data space recursively by maximizing an objective function such as Information Gain or minimizing impurity.\n",
        "\n",
        "At each node, the algorithm chooses the feature and threshold (for numeric features) or value (for categorical features) that best (decided upon information gain) splits the data.\n",
        "\n",
        "Let $D$ be a dataset and $A$ a feature. We compute the split quality (e.g., Information Gain) and recursively apply the same process to the subsets until a stopping criterion is met (we ca specify the max_depth we want, or zero impurity i.e., until the leaf node becomes completely homogeneous or pure)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **8. What is Pre-Pruning in Decision Trees?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Pre-Pruning** is a technique used to *cut* the tree growth **early** during its construction to prevent over-fitting.\n",
        "\n",
        "Common pre-pruning strategies include:\n",
        "- Limiting tree depth\n",
        "- Mentioning a minimum number of samples at a node\n",
        "- Setting a minimum information gain threshold\n",
        "\n",
        "By restricting tree complexity upfront, pre-pruning balances bias and variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **9. What is Post-Pruning in Decision Trees?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Post-Pruning** involves growing the full decision tree and *then trimming back* branches that have little statistical significance.\n",
        "\n",
        "This is typically done by evaluating the treeâ€™s performance on a validation dataset.\n",
        "\n",
        "Common techniques include cost-complexity pruning (used in CART), reduced error pruning, and weakest link pruning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **10. What is the difference between Pre-Pruning and Post-Pruning?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Feature           | Pre-Pruning                                 | Post-Pruning                                |\n",
        "|--------------------|----------------------------------------------|---------------------------------------------|\n",
        "| Timing             | Stops tree growth early                      | Prunes after full tree is built             |\n",
        "| Decision Basis     | Thresholds like depth, min samples, gain     | Validation set or cross-validation          |\n",
        "| Complexity Control | Proactive                                   | Reactive                                     |\n",
        "| Common Algorithms  | C4.5, ID3                                    | CART, Reduced Error Pruning                 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **11. What is a Decision Tree Regressor?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A **Decision Tree Regressor** is used for predicting continuous numerical values.\n",
        "\n",
        "Instead of class labels, the leaf nodes hold numeric values, and the tree splits the data by minimizing variance (in place of Information Gain) within each split.\n",
        "\n",
        "The goal is to partition the feature space such that the average squared error (MSE) of the predictions is minimized."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "551460b6",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **12. What are the advantages and disadvantages of Decision Trees?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Advantages:**\n",
        "- Easy to interpret and visualize\n",
        "- No need for feature scaling\n",
        "- Can handle both numerical (regressor) and categorical (classifier) data\n",
        "- Works well with non-linear data\n",
        "\n",
        "**Disadvantages:**\n",
        "- Prone to overfitting\n",
        "- Instability to small changes in data\n",
        "- Biased with imbalanced datasets\n",
        "- Less accurate compared to ensemble methods like Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **13. How does a Decision Tree handle missing values?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Decision Trees handle missing values using techniques such as:\n",
        "\n",
        "- **Surrogate Splits**: Find alternative splits using other correlated features.\n",
        "- **Imputation**: Replace missing values with mean/median/mode or predictive modeling.\n",
        "- **Splitting on presence**: Create an additional category or branch for missing values.\n",
        "\n",
        "Some implementations like CART and C4.5 directly support missing value handling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **14. How does a Decision Tree handle categorical features?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Categorical features are handled by evaluating splits for each category or grouping them:\n",
        "\n",
        "- For **binary splits**, the tree finds the best subset of categories for a decision boundary.\n",
        "- For **multiway splits**, a branch is created for each category.\n",
        "- Algorithms like CART use binary splits, while ID3 and C4.5 allow multiway splits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **15. What are some real-world applications of Decision Trees?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Medical Diagnosis**: Predicting disease based on symptoms\n",
        "- **Loan Approval**: Determining creditworthiness\n",
        "- **Customer Segmentation**: Marketing and targeting strategies\n",
        "- **Fraud Detection**: Identifying suspicious transactions\n",
        "- **Risk Assessment**: In insurance and financial domains\n",
        "- **Manufacturing**: Predictive maintenance and quality control"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
